{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70345039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, DownloadConfig,DatasetDict\n",
    "import polars as pl\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "34e852d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.8'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "010a8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DownloadConfig(\n",
    "    cache_dir=\"C:/Users/mike/.cache/huggingface\",\n",
    "    local_files_only=True,\n",
    "    force_download=False,\n",
    "    use_etag=True,\n",
    "    resume_download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7ef459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"coastalcph/multi_eurlex\", name=\"en\", split=\"train\", download_config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "38c67db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = temp_split[\"train\"]\n",
    "split_data = temp_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "val_dataset = split_data[\"train\"]\n",
    "test_dataset = split_data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "59bf8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\":train_dataset,\n",
    "    \"validation\":val_dataset,\n",
    "    \"test\":test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52098524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['celex_id', 'text', 'labels'],\n",
       "        num_rows: 44000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['celex_id', 'text', 'labels'],\n",
       "        num_rows: 5500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['celex_id', 'text', 'labels'],\n",
       "        num_rows: 5500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb3c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max string at: 2764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(343062, 719, np.float64(7150.912727272727), np.float64(16302.266859774212))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_len = [len(text) for text in test_data[\"text\"]]\n",
    "for i,j in enumerate(string_len):\n",
    "    if j == max(string_len):\n",
    "        print(f\"Max string at: {i}\")\n",
    "max(string_len), min(string_len), np.mean(string_len), np.std(string_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fcdcac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"train\"][:10][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bedd97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b15aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 17])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ea910b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 17],\n",
       " [17, 5, 6],\n",
       " [3, 17, 5],\n",
       " [2, 4, 3, 12, 18, 15],\n",
       " [17, 19, 5, 6, 18],\n",
       " [18, 15, 19, 6],\n",
       " [3, 17, 15],\n",
       " [4, 5, 14, 20, 15],\n",
       " [18, 19],\n",
       " [3, 0, 18, 6]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d9f7a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe854e92ddd4e42b257815240afc561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28907/2655867048.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = F.one_hot(torch.tensor(example[\"labels\"]), num_classes=21)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c924749ec4e2432f812811a51291e48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f3eec9d2ca431d96246091507aa2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def code_labels(example):\n",
    "    labels = F.one_hot(torch.tensor(example[\"labels\"]), num_classes=21)\n",
    "    example[\"labels\"]=labels.to(torch.float32)\n",
    "    return example\n",
    "    \n",
    "dataset_coded = dataset.map(code_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c3779622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_coded[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b104d4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.unique(np.concatenate(dataset[\"train\"][\"labels\"]))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0a66504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_code(label_list):\n",
    "    one_hot = np.zeros(len(labels),dtype=float)\n",
    "    one_hot[label_list] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a3c627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'celex_id': '32001R1840',\n",
       " 'text': 'Commission Regulation (EC) No 1840/2001\\nof 19 September 2001\\namending for the third time Regulation (EC) No 23/2001 laying down special measures for the beef sector that depart from the provisions of Regulation (EC) No 800/1999, Regulation (EEC) No 3719/88, Regulation (EC) No 1291/2000 and Regulation (EEC) No 1964/82\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES,\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Council Regulation (EC) No 1254/1999 of 17 May 1999 on the common organisation of the market in beef and veal(1), as last amended by Regulation (EC) No 1512/2001(2), and in particular Article 29(2)(a), Article 33(12) and Article 41 thereof,\\nWhereas:\\n(1) The health protection measures adopted by the authorities of certain non-member countries regarding exports of bovine animals and the meat of those animals in response to bovine spongiform encephalopathy have had serious economic consequences for exporters.\\n(2) The cases of foot-and-mouth disease that have occurred in several Member States have led to adoption of protective measures under Council Directive 90/425/EEC of 26 June 1990 concerning veterinary and zootechnical checks applicable in intra-Community trade in certain live animals and products with a view to the completion of the internal market(3), as last amended by Directive 92/118/EEC(4), and in particular Article 10 thereof, and under Council Directive 89/662/EEC of 11 December 1989 concerning veterinary checks in intra-Community trade with a view to the completion of the internal market(5), as last amended by Directive 92/118/EEC, and in particular Article 9 thereof.\\n(3) Commission Regulation (EC) No 23/2001(6), as last amended by Regulation (EC) No 908/2001(7), introduces measures to limit the serious consequences of those measures.\\n(4) The health protection measures adopted by certain non-member countries regarding Community exports are still in force and in certain cases have been strengthened. In view of this situation, certain time limits must be extended, although not beyond 31 December 2001.\\n(5) The measures provided for in this Regulation are in accordance with the opinion of the Management Committee for Beef and Veal,\\nHAS ADOPTED THIS REGULATION:\\nArticle 1\\nArticle 2 of Regulation (EC) No 23/2001 is replaced by the following: \"Article 2\\n1. At the holder\\'s request, export licences issued under Regulation (EC) No 1445/95 that were applied for by 30 March 2001 shall, if their validity did not expire before 1 November 2000, be cancelled and the security released.\\n2. On application by the exporter in the case of products for which by 30 March 2001:\\n- the customs export formalities had been completed or which had been placed under one of the customs control procedures referred to in Articles 4 and 5 of Regulation (EEC) No 565/80, the 60-day time limit for leaving the Community\\'s customs territory referred to in Article 32(1)(b)(i) of Regulation (EC) No 1291/2000 and Article 7(1) and Article 34(1) of Regulation (EC) No 800/1999 is extended to 31 December 2001,\\n- the customs export formalities had been completed but which had not yet left the Community\\'s customs territory or which had been placed under one of the customs control procedures referred to in Articles 4 and 5 of Regulation (EEC) No 565/80, the exporter shall repay any refund paid in advance and the various securities pertaining to the operations shall be released,\\n- the customs formalities had been completed and which had left the Community\\'s customs territory, they may be brought back and released for free circulation in the Community. The exporter shall repay any refund paid in advance and the various securities pertaining to the operations shall be released,\\n- the customs formalities had been completed and which had left the Community\\'s customs territory, they may be brought back to be placed under a suspensive procedure in a free zone, free warehouse or customs warehouse until 31 December 2001 before reaching their final destination; this shall not affect payment of the refund for the actual final destination or the security lodged in respect of the licence.\"\\nArticle 2\\nThis Regulation shall enter into force on the day following its publication in the Official Journal of the European Communities.\\nIt shall apply to operations for which a final decision has not yet been adopted.\\nThis Regulation shall be binding in its entirety and directly applicable in all Member States.\\nDone at Brussels, 19 September 2001.',\n",
       " 'labels': [3, 17]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c819d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"][0][\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c1765d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", use_fast=True)\n",
    "def tokenize_function(examples):\n",
    "    tokenized_input = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized_input[\"labels\"] = [one_hot_code(label) for label in examples[\"labels\"]]\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "659fb532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e634421839b4a0690f3a11957409186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokinezed_data = dataset[\"test\"].map(tokenize_function,remove_columns = [\"text\"] ,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7e143df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokinezed_data[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710dc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokinezed_data.remove_columns = [\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb891db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] commission regulation ( ec ) no 1840 / 2001 of 19 september 2001 amending for the third time regulation ( ec ) no 23 / 2001 laying down special measures for the beef sector that depart from the provisions of regulation ( ec ) no 800 / 1999, regulation ( eec ) no 3719 / 88, regulation ( ec ) no 1291 / 2000 and regulation ( eec ) no 1964 / 82 the commission of the european communities, having regard to the treaty establishing the european community, having regard to council regulation ( ec ) no 1254 / 1999 of 17 may 1999 on the common organisation of the market in beef and veal ( 1 ), as last amended by regulation ( ec ) no 1512 / 2001 ( 2 ), and in particular article 29 ( 2 ) ( a ), article 33 ( 12 ) and article 41 thereof, whereas : ( 1 ) the health protection measures adopted by the authorities of certain non - member countries regarding exports of bovine animals and the meat of those animals in response to bovine spongiform encephalopathy have had serious economic consequences for exporters. ( 2 ) the cases of foot - and - mouth disease that have occurred in several member states have led to adoption of protective measures under council directive 90 / 425 / eec of 26 june 1990 concerning veterinary and zootechnical checks applicable in intra - community trade in certain live animals and products with a view to the completion of the internal market ( 3 ), as last amended by directive 92 / 118 / eec ( 4 ), and in particular article 10 thereof, and under council directive 89 / 662 / eec of 11 december 1989 concerning veterinary checks in intra - community trade with a view to the completion of the internal market ( 5 ), as last amended by directive 92 / 118 / eec, and in particular article 9 thereof. ( 3 ) commission regulation ( ec ) no 23 / 2001 ( 6 ), as last amended by regulation ( ec ) no 908 / 2001 ( 7 ), introduces measures to limit the serious consequences of those measures. ( 4 ) the health protection measures adopted by certain non - member countries regarding community exports are still in force and in certain cases have been strengthened. in view of this situation, certain time limits must be extended, although not beyond 31 december 2001. ( 5 ) the measures provided for in this regulation are in accordance with the opinion of the management committee for beef and veal, has adopted this regulation : article 1 article 2 of regulation ( ec ) no 23 / 2001 is replaced by the following : \" article [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokinezed_data[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "640f2084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2565"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.decode(tokinezed_data[\"validation\"][2][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54510fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'100149': 'social questions', '100160': 'industry', '100148': 'finance', '100147': 'trade', '100152': 'business and competition', '100143': 'international relations', '100156': 'agriculture, forestry and fisheries', '100158': 'production, technology and research', '100154': 'transport', '100153': 'employment and working conditions', '100142': 'politics', '100145': 'law', '100150': 'education and communications', '100162': 'international organisations', '100159': 'energy', '100144': 'EUROPEAN UNION', '100151': 'science', '100157': 'agri-foodstuffs', '100161': 'geography', '100146': 'economics', '100155': 'environment'}\n",
      "{'social questions': '100149', 'industry': '100160', 'finance': '100148', 'trade': '100147', 'business and competition': '100152', 'international relations': '100143', 'agriculture, forestry and fisheries': '100156', 'production, technology and research': '100158', 'transport': '100154', 'employment and working conditions': '100153', 'politics': '100142', 'law': '100145', 'education and communications': '100150', 'international organisations': '100162', 'energy': '100159', 'EUROPEAN UNION': '100144', 'science': '100151', 'agri-foodstuffs': '100157', 'geography': '100161', 'economics': '100146', 'environment': '100155'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/eurovoc_descriptors.json\") as f:\n",
    "    eurovoc = json.load(f)\n",
    "\n",
    "labels = dataset[\"train\"].features[\"labels\"].feature.names\n",
    "descriptions = [eurovoc.get(label, {\"en\": \"Unknown\"})[\"en\"] for label in labels]\n",
    "\n",
    "id2labels = {label:desc for label,desc in zip(labels, descriptions)}\n",
    "label2id = {desc:label for label,desc in zip(labels, descriptions)}\n",
    "print(id2labels)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b52ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79e4e090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokinezed_data[\"train\"][0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed499591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokinezed_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "type(tokinezed_data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47c874a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokinezed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b9e78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\",problem_type = \"multi_label_classification\", num_labels=len(labels),id2label=labels,label2id=descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bee46e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"output_dir\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fe35fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score,roc_auc_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def multi_label_metriccs(predictions,labels,threshold=0.5):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.tensor(predictions))\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(labels, preds, average=\"micro\")\n",
    "    precision = precision_score(labels, preds, average=\"micro\")\n",
    "    recall = recall_score(labels, preds, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(labels, predictions, average=\"macro\", multi_class=\"ovr\")\n",
    "    return {\"f1\": f1, \"precision\": precision, \"recall\": recall, \"roc_auc\": roc_auc}\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    labels = p.label_ids\n",
    "    return multi_label_metriccs(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "486402e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokinezed_data[\"train\"],\n",
    "    eval_dataset=tokinezed_data[\"validation\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4d741b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokinezed_data[\"train\"][0][\"labels\"].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c347cfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6d7d2a041d4922852c343d077e1b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28907/470467744.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"labels\": torch.tensor(example[\"labels\"], dtype=torch.float32)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6019bf379c994ed28981144a24cafe88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f272aaef13447e28822298815a97a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert(example):\n",
    "    return {\"labels\": torch.tensor(example[\"labels\"], dtype=torch.float32)}\n",
    "tokenized_dataset = tokinezed_data.map(convert,load_from_cache_file=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1844f0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][:5][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1bc8a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Example: Convert a torch.LongTensor to torch.float32\n",
    "long_tensor = torch.tensor([1, 2, 3], dtype=torch.long)\n",
    "float_tensor = long_tensor.to(torch.float32)\n",
    "print(float_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d26e507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=tokenized_dataset[\"train\"][0][\"labels\"].type(torch.float32)\n",
    "sample.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e345b264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba59da007fbb44ccb8d0941433ced962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28907/3990998404.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"labels\": torch.tensor(example[\"labels\"], dtype=torch.float32)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8af05cab814748b69210ab4d705f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abd09ef3641472ba196b54274a321b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert(example):\n",
    "    # Convert the list of labels into a single torch.FloatTensor\n",
    "    return {\"labels\": torch.tensor(example[\"labels\"], dtype=torch.float32)}\n",
    "tokenized_dataset = tokenized_dataset.map(convert, batched=True, load_from_cache_file=False)\n",
    "tokenized_dataset[\"train\"][0][\"labels\"].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9c46616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence(feature=ClassLabel(names=['100149', '100160', '100148', '100147', '100152', '100143', '100156', '100158', '100154', '100153', '100142', '100145', '100150', '100162', '100159', '100144', '100151', '100157', '100161', '100146', '100155'], id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"].features[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27d49c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0][\"labels\"].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d1eb260",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column test not in the dataset. Current columns in the dataset: ['celex_id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: labels}\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# sample = convert(tokenized_dataset[\"train\"][0])\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# sample[\"labels\"].type()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenized_dataset = \u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.map(convert, batched=\u001b[38;5;28;01mFalse\u001b[39;00m, load_from_cache_file=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m tokenized_dataset[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m].type()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py:2761\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2759\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2761\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2762\u001b[39m formatted_output = format_table(\n\u001b[32m   2763\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2764\u001b[39m )\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/formatting/formatting.py:604\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    602\u001b[39m         _raise_bad_key_type(key)\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    606\u001b[39m     size = indices.num_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table.num_rows\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/datasets/formatting/formatting.py:541\u001b[39m, in \u001b[36m_check_valid_column_key\u001b[39m\u001b[34m(key, columns)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"Column test not in the dataset. Current columns in the dataset: ['celex_id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask']\""
     ]
    }
   ],
   "source": [
    "def convert(example):\n",
    "    # Convert labels to torch.FloatTensor\n",
    "    labels = example[\"labels\"].to(torch.float32)\n",
    "    return {\"labels\": labels}\n",
    "# sample = convert(tokenized_dataset[\"train\"][0])\n",
    "# sample[\"labels\"].type()\n",
    "\n",
    "tokenized_dataset = tokenized_dataset[\"test\"].map(convert, batched=False, load_from_cache_file=False)\n",
    "tokenized_dataset[\"labels\"][0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ccc8fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b5cadaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_dataset[\"validation\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c7fcd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0][\"labels\"].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc58744b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "label = dataset[\"train\"][0][\"labels\"]\n",
    "label_t = torch.tensor(label,dtype=torch.long)\n",
    "one_hot_code=F.one_hot(label_t,21)\n",
    "one_hot = one_hot_code.to(torch.float32)\n",
    "one_hot.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5e3c8f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2d4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
